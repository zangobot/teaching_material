{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "During this lab, we will refresh the notions about Homomorphic Encryption seen in class and create a simple Logistic Regression classifier which exploit this kind of encryption for carrying private computations!\n",
        "\n",
        "This lab is based on the tutorials that can be found on the [TenSEAL github project](https://github.com/OpenMined/TenSEAL/tree/main/tutorials)."
      ],
      "metadata": {
        "id": "k98PRqKXSrvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 00 - Homomorphic Encryption"
      ],
      "metadata": {
        "id": "nedJPxyMCBbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with recalling the definition of ***Homomorphism***:\n",
        "\n",
        "A homomorphism is a map between two algebraic structures of the same type (e.g. vector spaces), that preserves the operations of the structures. This means a map $f:\\mathcal{A}\\to \\mathcal{B}$ between two sets $\\mathcal{A}$, $\\mathcal{B}$ equipped with the same structure such that, if $\\star$ is an operation of the structure (supposed here, for simplification, to be a binary operation), then:\n",
        "$$\n",
        "f(A_1\\star A_2) = f(A_1)\\star f(A_2)\\;\\;\\;\\forall A_1,A_2\\in\\mathcal{A}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "dflL5LBc_tvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, suppose that $f$ is an encryption map that transform a plaintext which contains sensitive information into a cyphertext capable of completely masking it. This kind of encryption could allow us to work on cyphertexts seamelessly as we would perform the required operations on the original plaintexts!\n",
        "\n",
        "For example, lets consider the pseudo-code snippet below:\n",
        "```\n",
        "# ----- User has these private variables and a secet key\n",
        "private_x = 453\n",
        "private_y = 590\n",
        "secr_key = 42\n",
        "\n",
        "# ----- User encrypts the variables and send them to an untrusted server for computations\n",
        "encr_x = HE.encrypt(private_x, secr_key)\n",
        "encr_y = HE.encrypt(private_y, secr_key)\n",
        "send_to_remote_server(encr_x, encr_y)\n",
        "\n",
        "# ----- Server-side: compute the required operations and send the results back to the user\n",
        "encr_x, encr_y = receive_from_user()\n",
        "encr_res = encr_x + encr_y\n",
        "send_to_user(encr_res)\n",
        "\n",
        "# ----- User receive the results from server and check the correctness of the outsorced operations\n",
        "encr_res = receive_from_remote_server()\n",
        "decr_res = encr_sum.decrypt(secr_key)\n",
        "\n",
        "plain_sum = private_x + private_y\n",
        "\n",
        "plain_sum == decr_res\n",
        ">>> prints True\n",
        "```"
      ],
      "metadata": {
        "id": "-YUHBJ_RH0_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Historically, Homomorphic Encryption (from now on, HE) can be divided in three families, depending on the oparations and the number of computations allowed:\n",
        "* ***Partial HE*** benefits from an unlimited number of computations, but only\n",
        "one operation is allowed (e.g. summation, multiplication);\n",
        "* ***Somewhat HE*** allows for multiple operations but suffers from a limited number of computations due to an increasing amount of computations-derived noise;\n",
        "* ***Full HE*** allows both a multiple number of operations and an unlimited number of computations, but generally suffers from huge computational costs.\n",
        "\n"
      ],
      "metadata": {
        "id": "TMaYo01aJ5xB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial we will exploit the python library for HE [TenSEAL](https://github.com/OpenMined/TenSEAL) curated by [OpenMined](https://www.openmined.org/).\n",
        "TenSEAL is built on top of [Microsoft SEAL](https://github.com/Microsoft/SEAL), a C++ library implementing the BFV (for computations on integer number) and CKKS (on real numbers) homomorphic encryption schemes.\n",
        "\n",
        "**Let's get started!**"
      ],
      "metadata": {
        "id": "P9-fvgjcO0T9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 01 - Getting Familiar with TenSeal"
      ],
      "metadata": {
        "id": "eXhXvoj586rv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As always, install the library!"
      ],
      "metadata": {
        "id": "Hn9m7tk0Ernh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SXA0Lg9qB5ti"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import tenseal as ts\n",
        "except ImportError:\n",
        "    %pip install tenseal\n",
        "    import tenseal as ts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 01a - TenSEAL Context"
      ],
      "metadata": {
        "id": "Ax-jIfNUGs1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to instantiate a tenseal context, that is a special object that wrap all the encryption parameters for the selected scheme. In particulat, note how it holds both the secret and public encryption keys."
      ],
      "metadata": {
        "id": "Xr9Cg5pHFMpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(ts.Context)"
      ],
      "metadata": {
        "id": "PmQtobHNIRBN",
        "outputId": "8a2aa862-55a1-4f66-95d0-7b4d89463e05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class Context in module tenseal.enc_context:\n",
            "\n",
            "class Context(builtins.object)\n",
            " |  Context(scheme: tenseal.enc_context.SCHEME_TYPE = None, poly_modulus_degree: int = None, plain_modulus: int = None, coeff_mod_bit_sizes: List[int] = [], encryption_type: tenseal.enc_context.ENCRYPTION_TYPE = <ENCRYPTION_TYPE.ASYMMETRIC: <ENCRYPTION_TYPE.ASYMMETRIC: 0>>, n_threads: int = None, data: _tenseal_cpp.TenSEALContext = None)\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __copy__(self) -> 'Context'\n",
            " |  \n",
            " |  __init__(self, scheme: tenseal.enc_context.SCHEME_TYPE = None, poly_modulus_degree: int = None, plain_modulus: int = None, coeff_mod_bit_sizes: List[int] = [], encryption_type: tenseal.enc_context.ENCRYPTION_TYPE = <ENCRYPTION_TYPE.ASYMMETRIC: <ENCRYPTION_TYPE.ASYMMETRIC: 0>>, n_threads: int = None, data: _tenseal_cpp.TenSEALContext = None)\n",
            " |      Construct a context that holds keys and parameters needed for operating\n",
            " |      encrypted tensors using either BFV or CKKS scheme.\n",
            " |      \n",
            " |      Args:\n",
            " |          scheme : define the scheme to be used, either SCHEME_TYPE.BFV or SCHEME_TYPE.CKKS.\n",
            " |          poly_modulus_degree: The degree of the polynomial modulus, must be a power of two.\n",
            " |          plain_modulus: The plaintext modulus. Should not be passed when the scheme is CKKS.\n",
            " |          coeff_mod_bit_sizes: List of bit size for each coeffecient modulus.\n",
            " |              Can be an empty list for BFV, a default value will be given.\n",
            " |          encryption_type : define the encryption type to be used, either ENCRYPTION_TYPE.ASYMMETRIC, or ENCRYPTION_TYPE.SYMMETRIC.\n",
            " |          n_threads: define number of threads that shall be later used for parallel computation.\n",
            " |          data: A TenSEALContext to wrap. We won't construct a new object if it's passed.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A Context object.\n",
            " |  \n",
            " |  copy(self) -> 'Context'\n",
            " |  \n",
            " |  decryptor(self) -> tenseal.enc_context.Decryptor\n",
            " |  \n",
            " |  encryptor(self) -> tenseal.enc_context.Encryptor\n",
            " |  \n",
            " |  galois_keys(self) -> tenseal.enc_context.GaloisKeys\n",
            " |  \n",
            " |  generate_galois_keys(self, secret_key: tenseal.enc_context.SecretKey = None)\n",
            " |  \n",
            " |  generate_relin_keys(self, secret_key: tenseal.enc_context.SecretKey = None)\n",
            " |  \n",
            " |  has_galois_keys(self) -> bool\n",
            " |  \n",
            " |  has_public_key(self) -> bool\n",
            " |  \n",
            " |  has_relin_keys(self) -> bool\n",
            " |  \n",
            " |  has_secret_key(self) -> bool\n",
            " |  \n",
            " |  is_private(self) -> bool\n",
            " |  \n",
            " |  is_public(self) -> bool\n",
            " |  \n",
            " |  make_context_public(self, generate_galois_keys: bool = False, generate_relin_keys: bool = False)\n",
            " |      Drop secret part from the context. This is useful before sending the context for remote\n",
            " |      computation, as we don't want to send the secret-key that can be used to decrypt values.\n",
            " |      \n",
            " |      Args:\n",
            " |          generate_galois_keys: should we generate galois-keys before dropping the secret-key?\n",
            " |          generate_relin_keys: should we generate relin-keys before dropping the secret-key?\n",
            " |  \n",
            " |  public_key(self) -> tenseal.enc_context.PublicKey\n",
            " |  \n",
            " |  relin_keys(self) -> tenseal.enc_context.RelinKeys\n",
            " |  \n",
            " |  seal_context(self) -> tenseal.enc_context.SEALContext\n",
            " |  \n",
            " |  secret_key(self) -> tenseal.enc_context.SecretKey\n",
            " |  \n",
            " |  serialize(self, save_public_key: bool = True, save_secret_key: bool = False, save_galois_keys: bool = True, save_relin_keys: bool = True) -> bytes\n",
            " |      Serialize the context into a stream of bytes.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  load(data: bytes, n_threads: int = None) -> 'Context' from builtins.type\n",
            " |      Construct a context from a serialized buffer.\n",
            " |      \n",
            " |      Args:\n",
            " |          data : bytes buffer from the original context.\n",
            " |          n_threads: define number of threads that shall be later used for parallel computation.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A Context object.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  auto_mod_switch\n",
            " |  \n",
            " |  auto_relin\n",
            " |  \n",
            " |  auto_rescale\n",
            " |  \n",
            " |  data\n",
            " |      Get the wrapped low level TenSEALContext object\n",
            " |  \n",
            " |  global_scale\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = ts.context(ts.SCHEME_TYPE.BFV, poly_modulus_degree=4096, plain_modulus=1032193)\n",
        "context"
      ],
      "metadata": {
        "id": "Uq-FXWRrCJIS",
        "outputId": "0aa1b0d4-b495-48c8-94fd-a28e013eea4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tenseal.enc_context.Context at 0x7ff1d9e52970>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 01b - Encryption"
      ],
      "metadata": {
        "id": "4xbyxvVgGxiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have defined a TenSEAL context and the relative public key, we can encrypt a vector (or a matrix) of numbers (integer or real depending on the exploited encryption scheme)."
      ],
      "metadata": {
        "id": "LJzZeKiTGDWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "PmncdbNrrCLL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plain_vct = [1,2,3,4]\n",
        "plain_mtx = np.array([1,2,3,4,5,6]).reshape(2,-1)"
      ],
      "metadata": {
        "id": "5NNTsO0hTAbu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- encrypted vector\n",
        "encr_vct = ts.bfv_vector(context, plain_vct)\n",
        "encr_vct.size()"
      ],
      "metadata": {
        "id": "DYzKOS4h_jJk",
        "outputId": "e83a34a3-cae3-4af0-8070-3da060cc7245",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- encrypted matrix\n",
        "encr_mtx = ts.bfv_tensor(context, plain_mtx)\n",
        "encr_mtx.shape"
      ],
      "metadata": {
        "id": "JWVjhTibZaPk",
        "outputId": "6b14ddd1-9812-4798-bf63-8b6142dede61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 02c - Evaluation"
      ],
      "metadata": {
        "id": "NXxcrRlnG2HI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start doing some encrypted computations!"
      ],
      "metadata": {
        "id": "pfeoBv4gG-t6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Sum with plain vector\n",
        "pl_add = encr_vct + plain_vct\n",
        "pl_add.decrypt()"
      ],
      "metadata": {
        "id": "9fy1CstQG-8H",
        "outputId": "5f028449-eda7-490e-f11f-c3698ef64382",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4, 6, 8]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Multiplication with plain vector\n",
        "pl_add = encr_vct * plain_vct\n",
        "pl_add.decrypt()"
      ],
      "metadata": {
        "id": "6FWZkzWcTywx",
        "outputId": "1de6d868-5c19-4e07-ea97-6f1fcd79c98d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 4, 9, 16]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Multiplication with encrypted vector\n",
        "pl_add = encr_vct * encr_vct\n",
        "pl_add.decrypt()"
      ],
      "metadata": {
        "id": "xKs86HsRTyzL",
        "outputId": "11b993f2-6ad4-4e18-81d4-afe70764c237",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 4, 9, 16]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- matrix multiplication\n",
        "encr_mm = encr_mtx.mm(plain_mtx.T)\n",
        "encr_mm.decrypt().tolist()"
      ],
      "metadata": {
        "id": "kjWKXmdLX65R",
        "outputId": "71124636-8aa3-42ff-dc3c-b4152837d116",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[14, 32], [32, 77]]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 03c - Requirements"
      ],
      "metadata": {
        "id": "TYkhXqh8G2Jc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As seen in class, the major drawbacks of performing encrypted computation with HE is a big increase in the computational requirements, both in terms of time and memory! Let's try to measure the difference.\n",
        "\n",
        "In the example below we will simulate the memory and time impact of performing a backpropagation round of a data matrix with 50 samples and 10 features."
      ],
      "metadata": {
        "id": "BH9AWj7_G_nY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_data(n=50, d=10):\n",
        "    # data separable by the line `y = x`\n",
        "    x = np.random.randn(n, d)\n",
        "    y = (x[:, 0] >= x[:, 1]).astype(float)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "LEr0FCvdz-xA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Memory requirements\n",
        "import sys\n",
        "\n",
        "n = 50\n",
        "d = 10\n",
        "plain_mtx, plain_trg = random_data(n, d)\n",
        "print(f\"The memory impact of the plaintext matrix is {sys.getsizeof(plain_mtx)} bytes\")"
      ],
      "metadata": {
        "id": "Hm6sViDKG9i5",
        "outputId": "ed978bd0-ed2f-4f33-b4a1-80ed56fbb4ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The memory impact of the plaintext matrix is 4120 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poly_mod_degree = 8192\n",
        "coeff_mod_bit_sizes = [40, 20, 20, 40]\n",
        "# create TenSEALContext\n",
        "context = ts.context(ts.SCHEME_TYPE.CKKS,\n",
        "                     poly_modulus_degree=poly_mod_degree,\n",
        "                     coeff_mod_bit_sizes=coeff_mod_bit_sizes)\n",
        "context.global_scale = 2**20\n",
        "context.generate_galois_keys()"
      ],
      "metadata": {
        "id": "2bJIlCb8b8zK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encr_mtx = ts.ckks_tensor(context, plain_mtx)\n",
        "encr_trg = ts.ckks_tensor(context, plain_trg)\n",
        "\n",
        "byt = sum([sys.getsizeof(c.data()) for c in encr_mtx.ciphertext()])\n",
        "print(f\"The memory impact of the cyphertext matrix is {byt} bytes\")"
      ],
      "metadata": {
        "id": "2RtXYFhjd9ML",
        "outputId": "34985b5a-de29-4270-98de-a953683d2aa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The memory impact of the cyphertext matrix is 16000 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We discovered that the requirements of simply loading this encrypted data to memory are around 4 times the original plain matrix.\n",
        "This, of course, depend on the privacy strength that we want to guarantee for our data and algorithm: simpler algorithms or weaker privacy guarantees can work with smaller cyphertexts that will imply a smaller memory overhead.\n",
        "\n",
        "Let's measure the time impact for performing a simple linear backpropagation round."
      ],
      "metadata": {
        "id": "i0iVtBVfimD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "class SquareError:\n",
        "    @staticmethod\n",
        "    def risk(pred_out, desired_out):\n",
        "        return ((pred_out-desired_out)**2).sum()\n",
        "\n",
        "    @staticmethod\n",
        "    def delta(pred_out, desired_out):\n",
        "        return (pred_out-desired_out)\n",
        "\n",
        "class linearClassifier:\n",
        "    def __init__(self, n_feat, rnd_seed=42):\n",
        "        self.w = np.random.default_rng(rnd_seed).standard_normal(n_feat)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x.dot(self.w)\n",
        "        return res\n",
        "\n",
        "    def backpropagate(self, x, y, loss_criterion, lr=1e-1):\n",
        "        # predict\n",
        "        pred = self.forward(x)\n",
        "        # compute gradients\n",
        "        loss_delta = loss_criterion.delta(pred, y)\n",
        "        delta_w = x.transpose().dot(loss_delta)\n",
        "        if isinstance(delta_w, ts.CKKSTensor):\n",
        "            delta_w = np.array(delta_w.decrypt().tolist())\n",
        "        # update weights\n",
        "        self.w -= lr * delta_w"
      ],
      "metadata": {
        "id": "Fw_oDKAciliT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lin_cl = linearClassifier(d)\n",
        "loss = SquareError()\n",
        "print(f\"Pre-backpropagation loss: {loss.risk(lin_cl.forward(plain_mtx), plain_trg):.2f}\")\n",
        "t = time.time()\n",
        "lin_cl.backpropagate(plain_mtx, plain_trg, loss, lr=1e-2)\n",
        "t = time.time() - t\n",
        "print(f\"Post-backpropagation loss: {loss.risk(lin_cl.forward(plain_mtx), plain_trg):.2f}\")\n",
        "print(f\"\\nBackpropagation on plaintext took {t:.2e}\")"
      ],
      "metadata": {
        "id": "pNX9RNeP1mN_",
        "outputId": "9cbfbd2a-c515-4113-8fe8-241d6a109d39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-backpropagation loss: 341.82\n",
            "Post-backpropagation loss: 111.09\n",
            "\n",
            "Backpropagation on plaintext took 1.42e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lin_cl = linearClassifier(d)\n",
        "loss = SquareError()\n",
        "print(f\"Pre-backpropagation loss: {loss.risk(lin_cl.forward(encr_mtx), encr_trg).decrypt().tolist():.2f}\")\n",
        "t = time.time()\n",
        "lin_cl.backpropagate(encr_mtx, encr_trg, loss, lr=1e-2)\n",
        "t = time.time() - t\n",
        "print(f\"Post-backpropagation loss: {loss.risk(lin_cl.forward(encr_mtx), encr_trg).decrypt().tolist():.2f}\")\n",
        "print(f\"\\nBackpropagation on cyphertext took {t:.2e}\")"
      ],
      "metadata": {
        "id": "i4X4XFSY2P7f",
        "outputId": "7a9a961d-34e4-4713-89c6-517988b7d562",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-backpropagation loss: 470.44\n",
            "Post-backpropagation loss: 126.39\n",
            "\n",
            "Backpropagation on cyphertext took 2.34e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This simple linear example demonstrates that, even for a small number of samples, the overhead in computational time for the backpropagation is around 3-4 orders of magnitude!\n",
        "\n",
        "If you want to learn more on the theory behind the CKKS encryption scheme, [check this blog post by OpenMined](https://blog.openmined.org/ckks-explained-part-1-simple-encoding-and-decoding/)."
      ],
      "metadata": {
        "id": "tOkjJTVmEm_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 02 - Implementing a Logistic Regression classifier"
      ],
      "metadata": {
        "id": "MDAofV6kEfhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will try to implement a simple logistic regression classifier that is able to mask sensitive information thanks to the use of homomorphic encryption.\n",
        "\n",
        "We will build our classifier so that it will mimic the functionalities of a pytorch module."
      ],
      "metadata": {
        "id": "CsZSZnAXThYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 02a - The Dataset"
      ],
      "metadata": {
        "id": "-9HZRnWYrTDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But first we will download the data for our experiment: [Heart disease](https://www.kaggle.com/datasets/dileep070/heart-disease-prediction-using-logistic-regression?resource=download)\n",
        "\n",
        "The dataset is publically available on the Kaggle website, and it is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. The classification goal is to predict whether the patient has 10-year risk of future coronary heart disease (CHD).The dataset provides the patientsâ€™ information. It includes over 4,000 records and 15 attributes. Since it contains the patients information, it serves well as a sensitive application where we should mask the data and guarantee patients' privacy."
      ],
      "metadata": {
        "id": "OaglthVTT_Xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1b1cpGagEBnGSscdKK7G0CeyDrlQXUbem"
      ],
      "metadata": {
        "id": "dauY52x1A2sn",
        "outputId": "22d8f12c-063f-4301-e684-93e6aa07316a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1b1cpGagEBnGSscdKK7G0CeyDrlQXUbem\n",
            "To: /content/framingham.csv\n",
            "\r  0% 0.00/196k [00:00<?, ?B/s]\r100% 196k/196k [00:00<00:00, 91.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "dYj6YwUdU3Yb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_train_test(x, y, test_ratio=0.3, num_samples=100):\n",
        "    rng = np.random.default_rng(42)\n",
        "    idxs = rng.choice(len(x), size=num_samples, replace=False)\n",
        "    # delimiter between test and train data\n",
        "    delim = int(len(idxs) * test_ratio)\n",
        "    test_idxs, train_idxs = idxs[:delim], idxs[delim:]\n",
        "    return x[train_idxs], y[train_idxs], x[test_idxs], y[test_idxs]\n",
        "\n",
        "def heart_disease_data():\n",
        "    data = pd.read_csv(\"./framingham.csv\")\n",
        "    # drop rows with missing values\n",
        "    data = data.dropna()\n",
        "    # drop some features\n",
        "    data = data.drop(columns=[\"education\", \"currentSmoker\", \"BPMeds\", \"diabetes\", \"diaBP\", \"BMI\"])\n",
        "    # balance data\n",
        "    grouped = data.groupby('TenYearCHD')\n",
        "    data = grouped.apply(lambda x: x.sample(grouped.size().min(), random_state=73).reset_index(drop=True))\n",
        "    # extract labels\n",
        "    y = np.array(data[\"TenYearCHD\"].values, dtype=float)[:,None]\n",
        "    data = data.drop(columns=\"TenYearCHD\")\n",
        "    # standardize data\n",
        "    data = (data - data.mean()) / data.std()\n",
        "    x = np.array(data.values, dtype=float)\n",
        "    # reduce cardinality\n",
        "    return split_train_test(x, y, test_ratio=0.3, num_samples=150)"
      ],
      "metadata": {
        "id": "L8Y_qcgYTOgL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train, x_test, y_test = heart_disease_data()\n",
        "\n",
        "n_feat = x_train.shape[1]\n",
        "\n",
        "print(f\"x_train has shape: {x_train.shape}\")\n",
        "print(f\"y_train has shape: {y_train.shape}\")\n",
        "print(f\"x_test has shape: {x_test.shape}\")\n",
        "print(f\"y_test has shape: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "BnjLmVAmVo8X",
        "outputId": "d8bab88a-b85b-4b41-c543-fbca34dd3b94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train has shape: (105, 9)\n",
            "y_train has shape: (105, 1)\n",
            "x_test has shape: (45, 9)\n",
            "y_test has shape: (45, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 02b - Defining and training a standard Logistic Regression classifier"
      ],
      "metadata": {
        "id": "GxJ-Uhd5rXvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all we need to define the logistic (Sigmoid) function for performing the classification task and the loss function for training our model"
      ],
      "metadata": {
        "id": "c4tCmrBn2MNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid:\n",
        "    @staticmethod\n",
        "    def forward(x):\n",
        "        return 1/(1+np.e**(-x))\n",
        "\n",
        "    @staticmethod\n",
        "    def prime(x):\n",
        "        return Sigmoid.forward(x)*(1-Sigmoid.forward(x))\n",
        "\n",
        "class BinaryCrossEntropyLoss:\n",
        "    @staticmethod\n",
        "    def forward(pred, true):\n",
        "        return (true*np.log(pred) + (1-true)*np.log(1-pred)).mean()\n",
        "\n",
        "    @staticmethod\n",
        "    def delta(pred, true):\n",
        "        return pred - true # cross-entropy + sigmoid trick\n",
        "\n",
        "def sigmoidal_accuracy_score(pred, true):\n",
        "    return round((np.abs(pred - true) < .5).astype(float).mean(), 2)"
      ],
      "metadata": {
        "id": "UV5kA5jXZWwf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can define the class for our Logistic Regression classifier:"
      ],
      "metadata": {
        "id": "5wZkXzgn5NnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, n_features, rnd_seed=42):\n",
        "        rng = np.random.default_rng(rnd_seed)\n",
        "        self.weights = rng.standard_normal((n_features,1))\n",
        "        self.bias = np.array([0.])\n",
        "\n",
        "        self._delta_w = 0\n",
        "        self._delta_b = 0\n",
        "    \n",
        "    def forward(self, x):\n",
        "        raw = x.dot(self.weights) + self.bias\n",
        "        log = Sigmoid.forward(raw)\n",
        "        return log\n",
        "\n",
        "    def predict(self, x):\n",
        "        return (self.forward(x) > .5).astype(int)\n",
        "\n",
        "    def reset_gradients(self):\n",
        "        self._delta_w = 0\n",
        "        self._delta_b = 0\n",
        "\n",
        "    def train(self, tr_x, tr_y, vl_x, vl_y, loss, epochs=10, lr=1e-1):\n",
        "        acc = sigmoidal_accuracy_score(self.predict(vl_x), vl_y)\n",
        "        print(f\"Accuracy @ep 0 = {acc}\")\n",
        "\n",
        "        for ep in range(epochs):\n",
        "            # reset gradients counters\n",
        "            self.reset_gradients()\n",
        "\n",
        "            # predict and update gradients\n",
        "            pred = self.forward(tr_x)\n",
        "            loss_delta = loss.delta(pred, tr_y)\n",
        "            self._delta_w += tr_x.transpose().dot(loss_delta)\n",
        "            self._delta_b += loss_delta.sum()\n",
        "\n",
        "            # update parameters\n",
        "            self.weights -= lr * self._delta_w\n",
        "            self.bias -= lr * self._delta_b\n",
        "\n",
        "            acc = sigmoidal_accuracy_score(self.predict(vl_x), vl_y)\n",
        "            print(f\"Accuracy @ep {ep+1} = {acc}\")"
      ],
      "metadata": {
        "id": "RftC5CgzWmm1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lrc = LogisticRegression(n_feat)\n",
        "bce = BinaryCrossEntropyLoss()\n",
        "lrc.train(x_train, y_train, x_test, y_test, bce, epochs=20, lr=1e-2)"
      ],
      "metadata": {
        "id": "HcQgQRP8XKCQ",
        "outputId": "f0d71f7e-956b-4e89-8e0e-572cc2efec2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy @ep 0 = 0.29\n",
            "Accuracy @ep 1 = 0.4\n",
            "Accuracy @ep 2 = 0.38\n",
            "Accuracy @ep 3 = 0.44\n",
            "Accuracy @ep 4 = 0.49\n",
            "Accuracy @ep 5 = 0.56\n",
            "Accuracy @ep 6 = 0.62\n",
            "Accuracy @ep 7 = 0.67\n",
            "Accuracy @ep 8 = 0.64\n",
            "Accuracy @ep 9 = 0.64\n",
            "Accuracy @ep 10 = 0.64\n",
            "Accuracy @ep 11 = 0.64\n",
            "Accuracy @ep 12 = 0.67\n",
            "Accuracy @ep 13 = 0.69\n",
            "Accuracy @ep 14 = 0.69\n",
            "Accuracy @ep 15 = 0.69\n",
            "Accuracy @ep 16 = 0.69\n",
            "Accuracy @ep 17 = 0.69\n",
            "Accuracy @ep 18 = 0.69\n",
            "Accuracy @ep 19 = 0.69\n",
            "Accuracy @ep 20 = 0.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this particular configuration, the linear classifier is able to classify correctly almost 70% of the test samples, not bad!"
      ],
      "metadata": {
        "id": "hMMCC28C5XoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 02c - Evaluating a trained logistic regression classifier on encrypted data"
      ],
      "metadata": {
        "id": "jRlYyRvdrllD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before trying to train a classifier on private data from scracth we need to fix some issues.\n",
        "\n",
        "For example, remember how our encryption scheme for real number (CKKS) allows us to perform only addition and multiplcation? That means we can't use the very core structure of a Logistic regression classifier: the logistic function!\n",
        "\n",
        "Luckily, the Sigmoid can be approximated by a polynomial of degree 3 that behaves sufficiently well in the range \\[-5,5\\] (remember that we have standardized our data!):\n",
        "$$\n",
        "sigmoid(x) = 0.5 + 0.197 * x - 0.004 * x^3\n",
        "$$\n",
        "\\[from [Chen2018Logistic](https://eprint.iacr.org/2018/462.pdf)\\]."
      ],
      "metadata": {
        "id": "KawGb3T_5jBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ApproximatedSigmoid:\n",
        "    @staticmethod\n",
        "    def forward(x):\n",
        "        if isinstance(x, np.ndarray):\n",
        "            return np.polyval([0.5, 0.197, 0, -0.004], x)\n",
        "        return x.polyval([0.5, 0.197, 0, -0.004])\n",
        "    \n",
        "    @staticmethod\n",
        "    def prime(x):\n",
        "        if isinstance(x, np.ndarray):\n",
        "            return np.polyval([0.197, 0, -0.012], x)\n",
        "        return x.polyval([0.197, 0, -0.012])"
      ],
      "metadata": {
        "id": "lHOEKYU4X7LE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before defining a Logistic regression classifier that is able to be trained on encrypted data, we can define one that is able to load the parameters from a trained model (on plaintexts) and just make predictions (on encrypted data).\n",
        "This new model obviously will observe the constraints imposed by the encryption scheme and so it will exploit the polynomial approximation of the Sigmoid."
      ],
      "metadata": {
        "id": "I9Z-pJ0m8x23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncryptedLogisticRegression:\n",
        "    def __init__(self, trained_classifier):\n",
        "        self.weights = trained_classifier.weights\n",
        "        self.bias = trained_classifier.bias\n",
        "\n",
        "    def forward(self, x):\n",
        "        raw = x.dot(self.weights) + self.bias\n",
        "        log = ApproximatedSigmoid.forward(raw)\n",
        "        return log"
      ],
      "metadata": {
        "id": "1_j9HYC5r8dA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elrc = EncryptedLogisticRegression(lrc)"
      ],
      "metadata": {
        "id": "xVx7bOJCoTXG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poly_mod_degree = 8192\n",
        "coeff_mod_bit_sizes = [40, 21, 21, 21, 21, 21, 21, 40]\n",
        "context = ts.context(ts.SCHEME_TYPE.CKKS, poly_mod_degree, -1, coeff_mod_bit_sizes)\n",
        "context.global_scale = 2 ** 21\n",
        "context.generate_galois_keys()"
      ],
      "metadata": {
        "id": "en_PdMmq7Fj_"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Encrypt private data\n",
        "encr_x_test = ts.ckks_tensor(context, x_test)"
      ],
      "metadata": {
        "id": "Fo7UcO0HpsWp"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Forward encrypted data through the network\n",
        "encr_pred_ts = elrc.forward(encr_x_test)"
      ],
      "metadata": {
        "id": "4xi-gF4usnkJ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Decrypt and compute predictions\n",
        "decr_pred_ts = np.array(encr_pred_ts.decrypt().tolist())\n",
        "encr_pred_ts = (decr_pred_ts > .5).astype(int)\n",
        "\n",
        "encr_ts_acc = sigmoidal_accuracy_score(encr_pred_ts, y_test)\n",
        "print(f\"Encrypted test accuracy: {encr_ts_acc}\")"
      ],
      "metadata": {
        "id": "unLs0R1uv_7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6359eff-d232-42fe-f5f1-82e50e7c159f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encrypted test accuracy: 0.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that in this case the random noise added by the encryption scheme was not able to corrupt (or improve) the model average accuracy!"
      ],
      "metadata": {
        "id": "rGEJQYBMeEtL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 02d - Training a Logistic Regression classifier on private data"
      ],
      "metadata": {
        "id": "O90dgF5NsARr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can extend the class defined above so that the classifier can be trained on encrypted data from scratch."
      ],
      "metadata": {
        "id": "uojijISjfqED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncryptedLogisticRegression:\n",
        "    def __init__(self, n_features, rnd_seed=42):\n",
        "        rng = np.random.default_rng(rnd_seed)\n",
        "        self.weights = rng.standard_normal(n_features)\n",
        "        self.bias = np.array([0.])\n",
        "\n",
        "        self._delta_w = 0\n",
        "        self._delta_b = 0\n",
        "    \n",
        "    def forward(self, x):\n",
        "        raw = x.dot(self.weights) + self.bias\n",
        "        log = ApproximatedSigmoid.forward(raw)\n",
        "        return log\n",
        "\n",
        "    def predict(self, x):\n",
        "        return (self.forward(x) > .5).astype(int)\n",
        "\n",
        "    def reset_gradients(self):\n",
        "        self._delta_w = 0\n",
        "        self._delta_b = 0\n",
        "\n",
        "    def encrypt(self, context):\n",
        "        self.weights = ts.ckks_vector(context, self.weights)\n",
        "        self.bias = ts.ckks_vector(context, self.bias)\n",
        "\n",
        "    def decrypt(self):\n",
        "        self.weights = np.array(self.weights.decrypt())\n",
        "        self.bias = np.array(self.bias.decrypt())\n",
        "\n",
        "    def train(self, tr_x, tr_y, vl_x, vl_y, tr_context, loss, epochs=20, lr=1e-1):\n",
        "        acc = sigmoidal_accuracy_score(self.predict(vl_x)[:,None], vl_y)\n",
        "        print(f\"Accuracy @ep 0 = {acc}\")\n",
        "\n",
        "        for ep in range(epochs):\n",
        "            # reset gradients counters\n",
        "            self.reset_gradients()\n",
        "\n",
        "            # encrypt model parameters\n",
        "            self.encrypt(tr_context)\n",
        "\n",
        "            # predict and update gradients\n",
        "            for enc_x, enc_y in zip(tr_x, tr_y):\n",
        "                pred = self.forward(enc_x)\n",
        "                loss_delta = loss.delta(pred, enc_y)\n",
        "                self._delta_w += enc_x * loss_delta\n",
        "                self._delta_b += loss_delta\n",
        "\n",
        "            # update parameters\n",
        "            self.weights -= lr * self._delta_w\n",
        "            self.bias -= lr * self._delta_b\n",
        "\n",
        "            # decrypt model parameters\n",
        "            self.decrypt()\n",
        "\n",
        "            acc = sigmoidal_accuracy_score(self.predict(vl_x)[:,None], vl_y)\n",
        "            print(f\"Accuracy @ep {ep+1} = {acc}\")"
      ],
      "metadata": {
        "id": "SDqs24RKlSIN"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encr_x_train = [ts.ckks_vector(context, x) for x in x_train]\n",
        "encr_y_train = [ts.ckks_vector(context, y) for y in y_train]"
      ],
      "metadata": {
        "id": "dXFw6e_8DdOv"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elrc = EncryptedLogisticRegression(n_feat)\n",
        "elrc.train(encr_x_train, encr_y_train,\n",
        "           x_test, y_test,\n",
        "           context, bce,\n",
        "           epochs=20, lr=1e-2)"
      ],
      "metadata": {
        "id": "h5rBGVYEqqgK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8b4327e-4636-448a-c3cc-15a81f63d70d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy @ep 0 = 0.29\n",
            "Accuracy @ep 1 = 0.47\n",
            "Accuracy @ep 2 = 0.62\n",
            "Accuracy @ep 3 = 0.62\n",
            "Accuracy @ep 4 = 0.6\n",
            "Accuracy @ep 5 = 0.64\n",
            "Accuracy @ep 6 = 0.67\n",
            "Accuracy @ep 7 = 0.64\n",
            "Accuracy @ep 8 = 0.64\n",
            "Accuracy @ep 9 = 0.64\n",
            "Accuracy @ep 10 = 0.67\n",
            "Accuracy @ep 11 = 0.67\n",
            "Accuracy @ep 12 = 0.67\n",
            "Accuracy @ep 13 = 0.67\n",
            "Accuracy @ep 14 = 0.69\n",
            "Accuracy @ep 15 = 0.69\n",
            "Accuracy @ep 16 = 0.69\n",
            "Accuracy @ep 17 = 0.69\n",
            "Accuracy @ep 18 = 0.69\n",
            "Accuracy @ep 19 = 0.69\n",
            "Accuracy @ep 20 = 0.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the results above, we can see how the logistic regressor classifier can be trained on the encrypted data and achieve results comparable to the \"plaintext\" classifier despite the added encryption noise and computational overhead!"
      ],
      "metadata": {
        "id": "b_tXBEGOh61-"
      }
    }
  ]
}